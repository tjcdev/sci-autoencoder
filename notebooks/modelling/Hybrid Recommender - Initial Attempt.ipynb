{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Hybird Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from time import gmtime, strftime\n",
    "\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import keras.backend as K\n",
    "from scipy import sparse\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5 #int(sys.argv[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../data')\n",
    "sys.path.append('../../src/models')\n",
    "from recommenders.cf_recommender import CFRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoencoder to use\n",
    "autoencoder_model = 'train_autoencoder_0_deep2_users_projects' #str(sys.argv[2])\n",
    "dataSource = 'users_projects' # str(sys.argv[3])\n",
    "model = load_model('../../data/autoencoders/' + autoencoder_model + '.h5')\n",
    "projects = pd.read_pickle('../../data/processed/project_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_users_projects():\n",
    "    cf = pd.read_pickle('../../data/processed/cf_projects.pkl')\n",
    "    #cf = pd.read_pickle('data/processed/cf_profiles.pkl')\n",
    "    train_x = sparse.load_npz(\"../../data/processed/train_sparse.npz\")\n",
    "    val_x = sparse.load_npz(\"../../data/processed/val_sparse.npz\")\n",
    "    test_x = sparse.load_npz(\"../../data/processed/test_sparse.npz\")\n",
    "    train_labels = cf\n",
    "    val_labels = cf\n",
    "    test_labels = cf\n",
    "    return train_labels, train_x, val_labels, val_x, test_labels, test_x\n",
    "\n",
    "def load_profile_labels():\n",
    "    cf_profiles = pd.read_pickle('../../data/processed/cf_profiles.pkl')\n",
    "    return cf_profiles\n",
    "\n",
    "# Load out time consistent collaborative filtering data\n",
    "train_labels, train_x, val_labels, val_x, test_labels, test_x = load_users_projects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the content based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src.models.recommenders.content_recommender import ContentRecommender\n",
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_projects_tfidf(field):\n",
    "    # Load the full project data from the pickle file\n",
    "    content_projects = pd.read_pickle(\"../../data/processed/project_data\")\n",
    "\n",
    "    # Get the TF-IDF for the description fields\n",
    "    v = TfidfVectorizer()\n",
    "    desc_idf = v.fit_transform(projects[field])\n",
    "\n",
    "    # Train/Val/Test Split\n",
    "    content_test_split_idx = int(np.floor(desc_idf.shape[0] * 0.8))\n",
    "    content_val_split_idx = int(content_test_split_idx * 0.9)\n",
    "\n",
    "    content_train_x = desc_idf[:content_val_split_idx]\n",
    "    content_val_x = desc_idf[content_val_split_idx:content_test_split_idx]\n",
    "    content_test_x = desc_idf[content_test_split_idx:]\n",
    "\n",
    "    content_train_labels_idx = np.arange(0, content_val_split_idx)\n",
    "    content_val_labels_idx = np.arange(content_val_split_idx, content_test_split_idx)\n",
    "    content_test_labels_idx = np.arange(content_test_split_idx, desc_idf.shape[0])\n",
    "\n",
    "    content_train_labels = pd.DataFrame(projects['project_id'].iloc[:content_val_split_idx], index=content_train_labels_idx)\n",
    "    content_val_labels = pd.DataFrame(projects['project_id'].iloc[content_val_split_idx:content_test_split_idx], index=content_val_labels_idx)\n",
    "    content_test_labels = pd.DataFrame(projects['project_id'].iloc[content_test_split_idx:], index=content_test_labels_idx)\n",
    "\n",
    "    return content_train_labels, content_train_x, content_val_labels, content_val_x, content_test_labels, content_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = 'description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoencoder to use\n",
    "autoencoder_model = 'train_autoencoder_32_cdae_tfidf_description'\n",
    "autoencoder = load_model('../../data/autoencoders/' + autoencoder_model + '.h5')\n",
    "\n",
    "project_train_labels, project_train_x, project_val_labels, project_val_x, project_test_labels, project_test_x = load_projects_tfidf(field)\n",
    "\n",
    "# Generate the embeddings\n",
    "embed_model = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('embedding_layer').output)\n",
    "x = vstack([project_train_x, project_val_x, project_test_x]).tocsr()\n",
    "x_projects = project_train_labels + project_val_labels + project_test_labels\n",
    "\n",
    "# TODO: remove projects that are not in the train_labels from the CF model\n",
    "\n",
    "embedding_size = autoencoder.get_layer('embedding_layer').output_shape[2]\n",
    "other_x = np.array(x_projects.index, dtype=np.int32).reshape(len(x_projects), 1).flatten()\n",
    "embeddings = embed_model.predict(x=[x, other_x])\n",
    "embeddings = embeddings.reshape(len(x_projects), embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender\n"
     ]
    }
   ],
   "source": [
    "train = sparse.load_npz(\"../../data/processed/train.npz\")\n",
    "test = sparse.load_npz(\"../../data/processed/test.npz\")\n",
    "\n",
    "# Build our recommender and similarity matrix\n",
    "recommender = ContentRecommender()\n",
    "similarity_matrix = recommender.similarity(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the user\n",
    "# TODO: we can turn this into a loop over all users later\n",
    "profile_idx = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative Filtering Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative Filtering Predictions\n",
    "profile_col = np.squeeze(np.asarray(train_x.getcol(profile_idx).todense())).reshape(1,-1)\n",
    "labels = np.asarray(train_labels.index)\n",
    "# Make a prediction for \n",
    "predictions = model.predict([profile_col, labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content Based Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to coerce to DataFrame, shape must be (1281, 1): given (1781, 1781)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-bf61346ee4e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate the similarity between user and projects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muser_projects_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_projects\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_projects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# TODO: Remove items from the list that are already done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# We can do this through masking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cdea/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, other, axis, level, fill_value)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2018\u001b[0;31m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_align_method_FRAME\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cdea/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m_align_method_FRAME\u001b[0;34m(left, right, axis)\u001b[0m\n\u001b[1;32m   1974\u001b[0m                                  \u001b[0;34m\"must be {req_shape}: given {given_shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m                                  .format(req_shape=left.shape,\n\u001b[0;32m-> 1976\u001b[0;31m                                          given_shape=right.shape))\n\u001b[0m\u001b[1;32m   1977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to coerce to DataFrame, shape must be (1281, 1): given (1781, 1781)"
     ]
    }
   ],
   "source": [
    "# Calculate the similarity between user and projects\n",
    "user_projects_sim = np.sum(train_projects * similarity_matrix.values, axis=0) / num_projects\n",
    "\n",
    "# TODO: Remove items from the list that are already done\n",
    "# We can do this through masking\n",
    "projects_to_not_suggest_again = train_projects.nonzero()[1]\n",
    "user_projects_sim[projects_to_not_suggest_again] = -1\n",
    "\n",
    "# Order the projects by similarity\n",
    "similar_items = pd.DataFrame(user_projects_sim)\n",
    "similar_items.columns = ['similarity_score']\n",
    "similar_items['project_id'] = similarity_matrix.columns\n",
    "similar_items = similar_items.sort_values('similarity_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
